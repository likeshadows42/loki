{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy                as np\n",
    "import sqlalchemy           as sqla\n",
    "import matplotlib.image     as mpimg\n",
    "import matplotlib.pyplot    as plt\n",
    "import api.global_variables as glb\n",
    "\n",
    "from re                     import compile, IGNORECASE\n",
    "from tqdm                   import tqdm\n",
    "from uuid                   import UUID\n",
    "from IFR.api                import *\n",
    "from IFR.classes            import *\n",
    "from IFR.functions          import *\n",
    "from sklearn.cluster        import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggles / flags:\n",
    "build_n_save      = False # build and save face detectors & verifiers\n",
    "create_database   = False # create database\n",
    "save_new_database = True  # save database\n",
    "align             = True  # perform face alignment\n",
    "\n",
    "# Paths:\n",
    "saved_detectors = 'api/saved_models/detectors' # face detector save directory\n",
    "saved_verifiers = 'api/saved_models/verifiers' # face verifier save directory\n",
    "\n",
    "# SQLITE_DB_FP    = '/home/rpessoa/projects/loki/api/data/database/loki.sqlite' # full path of new database\n",
    "SQLITE_DB_FP    = 'api/data/database/loki_test.sqlite'\n",
    "img_path        = 'api/data/img'                  # image directory to be used\n",
    "\n",
    "# Other:\n",
    "load_detectors  = ['retinaface']\n",
    "load_verifiers  = ['ArcFace']\n",
    "\n",
    "use_detector    = 'retinaface'\n",
    "use_verifier    = 'ArcFace'\n",
    "\n",
    "normalization   = 'base'\n",
    "metric          = 'cosine'\n",
    "\n",
    "# DBSCAN\n",
    "dbscan_eps         = 0.5\n",
    "dbscan_min_samples = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds and saves face detectors and verifiers (depending on 'build_n_save')\n",
    "if build_n_save:\n",
    "    # All face detector and verifier names\n",
    "    detector_names = ['opencv', 'ssd', 'mtcnn', 'retinaface']\n",
    "    verifier_names = ['VGG-Face', 'Facenet', 'Facenet512', 'OpenFace',\n",
    "                      'DeepFace', 'DeepID' , 'ArcFace']\n",
    "\n",
    "    # Builds all face detectors and verifiers\n",
    "    detectors = batch_build_detectors(detector_names, show_prog_bar=True,\n",
    "                                        verbose=False)\n",
    "    verifiers = batch_build_verifiers(verifier_names, show_prog_bar=True,\n",
    "                                        verbose=False)\n",
    "\n",
    "    # Prints the number of face detectors and verifiers built\n",
    "    print('Number of detectors built:', len(detectors))\n",
    "    print('Number of verifiers built:', len(verifiers), '\\n')\n",
    "\n",
    "    # Saves each face detector model\n",
    "    for name, obj in detectors.items():\n",
    "        status = save_built_model(name, obj, saved_detectors, overwrite=True,\n",
    "                                    verbose=True)\n",
    "    print('')\n",
    "\n",
    "    # Saves each face verifier model\n",
    "    for name, obj in verifiers.items():\n",
    "        status = save_built_model(name, obj, saved_verifiers, overwrite=True,\n",
    "                                    verbose=True)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Loading / creating face detectors:\n",
      "[load_built_model] Loading model retinaface.pickle: failed! Reason: retinaface.pickle does not exist in api/saved_models/detectors\n",
      "[build_detector] Building retinaface: Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "success!\n",
      "\n",
      "\n",
      "> Detectors:\n",
      "{'retinaface': <tensorflow.python.eager.def_function.Function object at 0x2c0bd4a30>}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Loading / creating face detectors ---------------------\n",
    "\n",
    "# Loads (or creates) all face detectors\n",
    "print('  -> Loading / creating face detectors:')\n",
    "detector_models = init_load_detectors(load_detectors, saved_detectors)\n",
    "print('\\n> Detectors:', detector_models, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Loading / creating face verifiers:\n",
      "[load_built_model] Loading model ArcFace.pickle: success!\n",
      "\n",
      "> Verifiers:\n",
      "{'ArcFace': <keras.engine.functional.Functional object at 0x2e9473e80>}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Loading / creating face verifiers ---------------------\n",
    "\n",
    "# Loads (or creates) all face verifiers\n",
    "print('  -> Loading / creating face verifiers:')\n",
    "verifier_models = init_load_verifiers(load_verifiers, saved_verifiers)\n",
    "print('\\n> Verifiers:', verifier_models, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Loading / creating database: success!\n",
      "\n",
      "  -> Loading / creating session: success!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tries to load a database if it exists. If not, create a new one.\n",
    "print('  -> Loading / creating database: ', end='')\n",
    "glb.sqla_engine = load_database(SQLITE_DB_FP)\n",
    "if glb.sqla_engine is None:\n",
    "    raise AssertionError('Failed to load or create database!')\n",
    "else:\n",
    "    print('success!')\n",
    "print('')\n",
    "\n",
    "# Tries to load a session if it exists. If not, create a new one.\n",
    "print('  -> Loading / creating session: ', end='')\n",
    "glb.sqla_session = start_session(glb.sqla_engine)\n",
    "if glb.sqla_session is None:\n",
    "    raise AssertionError('Failed to create session!')\n",
    "else:\n",
    "    print('success!')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_faces_from_dir(img_dir, detector_models, verifier_models, session,\n",
    "                        detector_name='retinaface', verifier_names=['ArcFace'],\n",
    "                        normalization='base', align=True, verbose=False):\n",
    "    \"\"\"\n",
    "    TODO: Documentation\n",
    "    \"\"\"\n",
    "    # Initializes records (which will be a list of FaceReps)\n",
    "    records = []\n",
    "\n",
    "    # Assuming img_dir is a directory containing images\n",
    "    img_paths = get_image_paths(img_dir)\n",
    "    img_paths.sort()\n",
    "\n",
    "    # No images found, do something about it\n",
    "    if len(img_paths) == 0:\n",
    "        # Does something about the fact that there are no images in the\n",
    "        # directory - for now just raise an assertion error\n",
    "        raise AssertionError('No images in the directory specified')\n",
    "\n",
    "    # Creates the progress bar\n",
    "    n_imgs = len(img_paths)\n",
    "    pbar   = tqdm(range(0, n_imgs), desc='Processing face images',\n",
    "                    disable=(not show_prog_bar))\n",
    "\n",
    "    # If auto grouping is True, then initialize the embeddings list\n",
    "    if auto_grouping:\n",
    "        embds = []\n",
    "\n",
    "    # Loops through each image in the 'img_dir' directory\n",
    "    for index, i, img_path in zip(pbar, range(0, n_imgs), img_paths):\n",
    "        # Detects faces\n",
    "        output = do_face_detection(img_path, detector_models=detector_models,\n",
    "                                    detector_name=detector_name, align=align,\n",
    "                                    verbose=verbose)\n",
    "\n",
    "        # Calculates the deep neural embeddings for each face image in outputs\n",
    "        embeddings = calc_embeddings(output['faces'], verifier_models,\n",
    "                                     verifier_names=verifier_names,\n",
    "                                     normalization=normalization)\n",
    "\n",
    "        # Loops through each (region, embedding) pair and create a record\n",
    "        # (FaceRep object)\n",
    "        for region, cur_embds in zip(output['regions'], embeddings):\n",
    "            # id        - handled by sqlalchemy\n",
    "            # person_id - dont now exactly how to handle this (sqlalchemy?)\n",
    "            # image_name_orig = img_path.split('/')[-1]\n",
    "            # image_fp_orig   = img_path\n",
    "            # image_name      = ''   # currently not being used in this approach\n",
    "            # image_fp        = ''   # currently not being used in this approach\n",
    "            # group_no        = -1   # will this be used? because person_id will be used instead I believe\n",
    "            # region          = region\n",
    "            # embeddings      = cur_embds\n",
    "            record = FaceRep(image_name_orig=img_path.split('/')[-1],\n",
    "                        image_name='', image_fp_orig=img_path,\n",
    "                        image_fp='', group_no=-1, region=region,\n",
    "                        embeddings=cur_embds)\n",
    "            \n",
    "            session.add(record)\n",
    "            records.append(record)\n",
    "\n",
    "            # If auto grouping is True, then store each calculated embedding\n",
    "            if auto_grouping:\n",
    "                embds.append(cur_embds[verifier_names[0]])\n",
    "\n",
    "\n",
    "    # Clusters Representations together using the DBSCAN algorithm\n",
    "    if auto_grouping:\n",
    "        # Clusters embeddings using DBSCAN algorithm\n",
    "        results = DBSCAN(eps=eps, min_samples=min_samples,\n",
    "                         metric=metric).fit(embds)\n",
    "\n",
    "        # Loops through each label and updates the 'group_no' attribute of each\n",
    "        # record IF group_no != -1 (because -1 is already the default value and\n",
    "        # means \"no group\")\n",
    "        for i, lbl in enumerate(results.labels_):\n",
    "            if lbl == -1:\n",
    "                continue\n",
    "            else:\n",
    "                records[i].group_no = lbl\n",
    "\n",
    "    # Loops through each record and add them to the global session\n",
    "    for record in records:\n",
    "        glb.sqla_session.add(record)\n",
    "    \n",
    "    # Return representation database\n",
    "    return records\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def process_faces_from_dir2(img_dir, detector_models, verifier_models,\n",
    "                        detector_name='retinaface', verifier_names=['ArcFace'],\n",
    "                        normalization='base', align=True, verbose=False):\n",
    "    \"\"\"\n",
    "    TODO: Documentation\n",
    "    \"\"\"\n",
    "    # Initializes records (which will be a list of FaceReps)\n",
    "    records = []\n",
    "\n",
    "    # Assuming img_dir is a directory containing images\n",
    "    img_paths = get_image_paths(img_dir)\n",
    "    img_paths.sort()\n",
    "\n",
    "    # No images found, do something about it\n",
    "    if len(img_paths) == 0:\n",
    "        # Does something about the fact that there are no images in the\n",
    "        # directory - for now just raise an assertion error\n",
    "        raise AssertionError('No images in the directory specified')\n",
    "\n",
    "    # Creates the progress bar\n",
    "    n_imgs = len(img_paths)\n",
    "    pbar   = tqdm(range(0, n_imgs), desc='Processing face images',\n",
    "                    disable=(not show_prog_bar))\n",
    "\n",
    "    # If auto grouping is True, then initialize the embeddings list\n",
    "    if auto_grouping:\n",
    "        embds = []\n",
    "\n",
    "    # Loops through each image in the 'img_dir' directory\n",
    "    for index, i, img_path in zip(pbar, range(0, n_imgs), img_paths):\n",
    "        # Detects faces\n",
    "        output = do_face_detection(img_path, detector_models=detector_models,\n",
    "                                    detector_name=detector_name, align=align,\n",
    "                                    verbose=verbose)\n",
    "\n",
    "        # Calculates the deep neural embeddings for each face image in outputs\n",
    "        embeddings = calc_embeddings(output['faces'], verifier_models,\n",
    "                                     verifier_names=verifier_names,\n",
    "                                     normalization=normalization)\n",
    "\n",
    "        # Loops through each (region, embedding) pair and create a record\n",
    "        # (FaceRep object)\n",
    "        for region, cur_embds in zip(output['regions'], embeddings):\n",
    "            # id        - handled by sqlalchemy\n",
    "            # person_id - dont now exactly how to handle this (sqlalchemy?)\n",
    "            # image_name_orig = img_path.split('/')[-1]\n",
    "            # image_fp_orig   = img_path\n",
    "            # image_name      = ''   # currently not being used in this approach\n",
    "            # image_fp        = ''   # currently not being used in this approach\n",
    "            # group_no        = -1\n",
    "            # region          = region\n",
    "            # embeddings      = cur_embds\n",
    "            record = (img_path.split('/')[-1], '', img_path, '', -1, region,\n",
    "                        cur_embds)\n",
    "            \n",
    "            records.append(record)\n",
    "\n",
    "            # If auto grouping is True, then store each calculated embedding\n",
    "            if auto_grouping:\n",
    "                embds.append(cur_embds[verifier_names[0]])\n",
    "\n",
    "\n",
    "    # Clusters Representations together using the DBSCAN algorithm\n",
    "    if auto_grouping:\n",
    "        # Clusters embeddings using DBSCAN algorithm\n",
    "        results = DBSCAN(eps=eps, min_samples=min_samples,\n",
    "                         metric=metric).fit(embds)\n",
    "\n",
    "        # Loops through each label and updates the 'group_no' attribute of each\n",
    "        # record IF group_no != -1 (because -1 is already the default value and\n",
    "        # means \"no group\")\n",
    "        for i, lbl in enumerate(results.labels_):\n",
    "            if lbl == -1:\n",
    "                continue\n",
    "            else:\n",
    "                records[i].group_no = lbl\n",
    "    \n",
    "    # Return representation database\n",
    "    return records\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "test_dir       = 'api/data/img'\n",
    "detector_name  = 'retinaface'\n",
    "verifier_names = ['ArcFace']\n",
    "align          = True\n",
    "show_prog_bar  = False\n",
    "tags           = []\n",
    "uids           = []\n",
    "normalization  = 'base'\n",
    "auto_grouping  = True\n",
    "eps            = 0.5\n",
    "min_samples    = 2\n",
    "metric         = 'cosine'\n",
    "verbose        = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[do_face_detection] Loading face detector: success!\n",
      "[do_face_detection] Detecting faces: success!\n",
      "[do_face_detection] Loading face detector: success!\n",
      "[do_face_detection] Detecting faces: success!\n"
     ]
    }
   ],
   "source": [
    "# This uses 'process_faces_from_dir' function\n",
    "do_approach_1 = True\n",
    "if do_approach_1:\n",
    "\n",
    "    records = process_faces_from_dir(test_dir, detector_models, verifier_models,\n",
    "                    glb.sqla_session, detector_name=detector_name, align=align,\n",
    "                    verifier_names=verifier_names, normalization=normalization,\n",
    "                    verbose=verbose)\n",
    "    glb.sqla_session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses 'process_faces_from_dir2' function\n",
    "do_approach_2 = False\n",
    "if do_approach_2:\n",
    "    records = process_faces_from_dir2(test_dir, detector_models,\n",
    "                    verifier_models, detector_name=detector_name, align=align,\n",
    "                    verifier_names=verifier_names, normalization=normalization,\n",
    "                    verbose=verbose)\n",
    "\n",
    "    reps = []\n",
    "    for i, record in enumerate(records):\n",
    "        reps.append(FaceRep(image_name_orig = record[0], image_name = record[1],\n",
    "                            image_fp_orig   = record[2], image_fp   = record[3],\n",
    "                            group_no        = record[4], region     = record[5],\n",
    "                            embeddings      = record[6]))\n",
    "        glb.sqla_session.add(reps[i])\n",
    "    glb.sqla_session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b8b4124e34c7aef84f71e2d1adc63dc40a11e5c939516b2a86977f9bc5888db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py3.8loki')",
   "language": "python",
   "name": "python3812jvsc74a57bd0c36595950957ef63cad889e648a471521abdf78ff754317594c451e2443d2095"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
