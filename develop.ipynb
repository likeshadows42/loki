{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy                as np\n",
    "import sqlalchemy           as sqla\n",
    "import matplotlib.image     as mpimg\n",
    "import matplotlib.pyplot    as plt\n",
    "import api.global_variables as glb\n",
    "\n",
    "from re                     import compile, IGNORECASE\n",
    "from tqdm                   import tqdm\n",
    "from uuid                   import UUID\n",
    "from filecmp                import cmp\n",
    "from IFR.api                import *\n",
    "from IFR.classes            import *\n",
    "from IFR.functions          import *\n",
    "from sklearn.cluster        import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggles / flags:\n",
    "build_n_save      = False # build and save face detectors & verifiers\n",
    "create_database   = False # create database\n",
    "save_new_database = True  # save database\n",
    "align             = True  # perform face alignment\n",
    "\n",
    "# Paths:\n",
    "saved_detectors = 'api/saved_models/detectors' # face detector save directory\n",
    "saved_verifiers = 'api/saved_models/verifiers' # face verifier save directory\n",
    "\n",
    "SQLITE_DB_FP    = 'api/data/database/loki_test.sqlite' # full path of new database\n",
    "img_path        = 'api/data/img'                       # image directory to be used\n",
    "\n",
    "# Other:\n",
    "load_detectors  = ['retinaface']\n",
    "load_verifiers  = ['ArcFace']\n",
    "\n",
    "use_detector    = 'retinaface'\n",
    "use_verifier    = 'ArcFace'\n",
    "\n",
    "normalization   = 'base'\n",
    "metric          = 'cosine'\n",
    "\n",
    "# DBSCAN\n",
    "dbscan_eps         = 0.5\n",
    "dbscan_min_samples = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds and saves face detectors and verifiers (depending on 'build_n_save')\n",
    "if build_n_save:\n",
    "    # All face detector and verifier names\n",
    "    detector_names = ['opencv', 'ssd', 'mtcnn', 'retinaface']\n",
    "    verifier_names = ['VGG-Face', 'Facenet', 'Facenet512', 'OpenFace',\n",
    "                      'DeepFace', 'DeepID' , 'ArcFace']\n",
    "\n",
    "    # Builds all face detectors and verifiers\n",
    "    detectors = batch_build_detectors(detector_names, show_prog_bar=True,\n",
    "                                        verbose=False)\n",
    "    verifiers = batch_build_verifiers(verifier_names, show_prog_bar=True,\n",
    "                                        verbose=False)\n",
    "\n",
    "    # Prints the number of face detectors and verifiers built\n",
    "    print('Number of detectors built:', len(detectors))\n",
    "    print('Number of verifiers built:', len(verifiers), '\\n')\n",
    "\n",
    "    # Saves each face detector model\n",
    "    for name, obj in detectors.items():\n",
    "        status = save_built_model(name, obj, saved_detectors, overwrite=True,\n",
    "                                    verbose=True)\n",
    "    print('')\n",
    "\n",
    "    # Saves each face verifier model\n",
    "    for name, obj in verifiers.items():\n",
    "        status = save_built_model(name, obj, saved_verifiers, overwrite=True,\n",
    "                                    verbose=True)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Loading / creating face detectors:\n",
      "[load_built_model] Loading model retinaface.pickle: failed! Reason: retinaface.pickle does not exist in api/saved_models/detectors\n",
      "[build_detector] Building retinaface: success!\n",
      "\n",
      "\n",
      "> Detectors:\n",
      "{'retinaface': <tensorflow.python.eager.def_function.Function object at 0x7f413466d4f0>}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Loading / creating face detectors ---------------------\n",
    "\n",
    "# Loads (or creates) all face detectors\n",
    "print('  -> Loading / creating face detectors:')\n",
    "detector_models = init_load_detectors(load_detectors, saved_detectors)\n",
    "print('\\n> Detectors:', detector_models, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Loading / creating face verifiers:\n",
      "[load_built_model] Loading model ArcFace.pickle: success!\n",
      "\n",
      "> Verifiers:\n",
      "{'ArcFace': <keras.engine.functional.Functional object at 0x7f413466db80>}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Loading / creating face verifiers ---------------------\n",
    "\n",
    "# Loads (or creates) all face verifiers\n",
    "print('  -> Loading / creating face verifiers:')\n",
    "verifier_models = init_load_verifiers(load_verifiers, saved_verifiers)\n",
    "print('\\n> Verifiers:', verifier_models, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Loading / creating database: success!\n",
      "\n",
      "  -> Loading / creating session: success!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tries to load a database if it exists. If not, create a new one.\n",
    "print('  -> Loading / creating database: ', end='')\n",
    "glb.sqla_engine = load_database(SQLITE_DB_FP)\n",
    "if glb.sqla_engine is None:\n",
    "    raise AssertionError('Failed to load or create database!')\n",
    "else:\n",
    "    print('success!')\n",
    "print('')\n",
    "\n",
    "# Tries to load a session if it exists. If not, create a new one.\n",
    "print('  -> Loading / creating session: ', end='')\n",
    "glb.sqla_session = start_session(glb.sqla_engine)\n",
    "if glb.sqla_session is None:\n",
    "    raise AssertionError('Failed to create session!')\n",
    "else:\n",
    "    print('success!')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dir       = 'api/data/img'\n",
    "detector_name  = 'retinaface'\n",
    "verifier_names = ['ArcFace']\n",
    "align          = True\n",
    "show_prog_bar  = False\n",
    "tags           = []\n",
    "uids           = []\n",
    "normalization  = 'base'\n",
    "auto_grouping  = True\n",
    "eps            = 0.5\n",
    "min_samples    = 2\n",
    "metric         = 'cosine'\n",
    "pct            = 0.02\n",
    "check_models   = False\n",
    "verbose        = False\n",
    "image_dir      = glb.IMG_DIR\n",
    "auto_rename    = True\n",
    "\n",
    "glb.DEBUG      = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import delete\n",
    "\n",
    "# Params structure\n",
    "params = CreateDatabaseParams(detector_name  = detector_name,\n",
    "                              verifier_names = verifier_names,\n",
    "                              align          = align,\n",
    "                              normalization  = normalization,\n",
    "                              auto_grouping  = auto_grouping,\n",
    "                              eps            = eps,\n",
    "                              min_samples    = min_samples,\n",
    "                              metric         = metric,\n",
    "                              pct            = pct,\n",
    "                              check_models   = check_models,\n",
    "                              verbose        = verbose)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def repopulate_temp_file_table(tpaths):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # First, tries to clear everything in the 'proc_files_temp' table\n",
    "    try:\n",
    "        stmt = delete(ProcessedFilesTemp)\n",
    "        glb.sqla_session.execute(stmt)\n",
    "        glb.sqla_session.commit()\n",
    "    except Exception as excpt:\n",
    "        glb.sqla_session.rollback()\n",
    "        print(\"Error when clearing 'proc_files_temp' table\",\n",
    "             f'(reason: {excpt})')\n",
    "        return True\n",
    "\n",
    "    if glb.DEBUG:\n",
    "        print(\"Populating 'proc_files_temp' table\")\n",
    "\n",
    "    # Loops through each temporary path in 'tpaths'\n",
    "    for tpath in tpaths:\n",
    "        # Adds each file name and size to the 'proc_files_temp' table\n",
    "        glb.sqla_session.add(ProcessedFilesTemp(\n",
    "                                        filename=tpath[tpath.rindex('/')+1:],\n",
    "                                        filesize=os.path.getsize(tpath))\n",
    "                            )\n",
    "\n",
    "    # Commits the changes\n",
    "    if glb.DEBUG:\n",
    "        print('Committing newly added temporary files')\n",
    "    glb.sqla_session.commit()\n",
    "\n",
    "    return False\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def file_is_not_unique2(fpath, proc_qry=None):\n",
    "    \"\"\"\n",
    "    TODO: Update documentation\n",
    "    \"\"\"\n",
    "    # Initializes 'is_not_unique' flag\n",
    "    is_not_unique = False\n",
    "\n",
    "    # Obtains the processed files from the ProcessedFiles table if it is not\n",
    "    # provided by the user (i.e. proc_qry is None)\n",
    "    if proc_qry is None:\n",
    "        proc_qry = glb.sqla_session.query(ProcessedFiles)\n",
    "\n",
    "    # Creates a subquery to find if there is/are file(s) in the ProcessedFiles\n",
    "    # table with the same file size and determines the number of files with the\n",
    "    # same size\n",
    "    subqry   = proc_qry.filter(\n",
    "                        ProcessedFiles.filesize.like(os.path.getsize(fpath))\n",
    "                              )\n",
    "    n_subqry = len(subqry.all())\n",
    "\n",
    "    # Checks if there is at least 1 matching file in the subquery\n",
    "    if n_subqry > 0:\n",
    "        # Loops through each matching file\n",
    "        for j in range(0, n_subqry):\n",
    "            # Determines if the files are the same (and should be skipped)\n",
    "            is_not_unique = img_files_are_same(fpath, subqry.all()[j].filepath)\n",
    "                                \n",
    "            # Current file matches another one. It's not unique so no need to\n",
    "            # continue this loop\n",
    "            if is_not_unique:\n",
    "                break\n",
    "\n",
    "    return is_not_unique\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def process_image_zip_file2(myfile, image_dir, auto_rename=True,\n",
    "                            valid_exts=['.jpg', '.png', '.npy']):\n",
    "    \"\"\"\n",
    "    Processes a zip file containing image files. The zip file ('myfile') is\n",
    "    assumed to have only valid image files (i.e. '.jpg', '.png', etc).\n",
    "    \n",
    "    The contents of the zip file are extracted to a named temporary directory.\n",
    "    Then each file is checked to see if they have already been processed (exists\n",
    "    with the same file name and size in the ProcessedFiles table of the\n",
    "    database) OR if they are duplicate files. A file is considered a duplicate\n",
    "    if there is at least one file in the 'image_dir' directory that:\n",
    "        \n",
    "        1. has the same file size (checked via filecmp.cmp(..., shallow=False))\n",
    "        2. has the same contents (checked via filecmp.cmp(..., shallow=False))\n",
    "        3. has the same image width and height (checked via imagesize)\n",
    "    \n",
    "    An existing file or duplicate file is ignored during the extraction process.\n",
    "    If 'auto_rename' is True, then each unique file with the same name as a file\n",
    "    in 'image_dir' directory gets renamed to a unique identifier using uuid4()\n",
    "    from the uuid library. If, however, 'auto_rename' is False then the file is\n",
    "    also skipped despite being a unique file.\n",
    "\n",
    "    Finally, all unique (possibly renamed) files are moved from the temporary\n",
    "    directory to the 'image_dir' directory, and the temporary directory is\n",
    "    deleted.\n",
    "\n",
    "    Effectively, this function attempts to extract only unique (non-existing)\n",
    "    image files from the zip file provided and rename them if necessary.\n",
    "\n",
    "    Inputs:\n",
    "        1. myfile      - zip file obtained through FastAPI [zip file].\n",
    "\n",
    "        2. image_dir   - path to directory in which the extracted images will be\n",
    "                            saved to [string].\n",
    "\n",
    "        3. auto_rename - toggles between automatic renaming of image files with\n",
    "                            a non-unique name [boolean, default=True].\n",
    "\n",
    "    Output:\n",
    "        1. list with the paths of each image file that was skipped [list of\n",
    "            strings].\n",
    "\n",
    "    Signature:\n",
    "        skipped_files = process_image_zip_file(myfile, image_dir,\n",
    "                                                auto_rename=True)\n",
    "    \"\"\"\n",
    "    # Create temporary directory and extract all files to it\n",
    "    with TemporaryDirectory(prefix=\"create_database_from_zip-\") as tempdir:\n",
    "        # with ZipFile(BytesIO(myfile.file.read()), 'r') as myzip:  # uncomment after development\n",
    "        with ZipFile(myfile, 'r') as myzip:                # remove after development\n",
    "            # Extracts all files in the zip folder\n",
    "            myzip.extractall(tempdir)\n",
    "            \n",
    "            # Obtains all file names, temporary file names and temporary file\n",
    "            # paths. Also initializes skipped_files list\n",
    "            skipped_files = []\n",
    "            all_fnames = [name.split('/')[-1] for name in os.listdir(image_dir)]\n",
    "            all_tnames = [name.split('/')[-1] for name in os.listdir(tempdir)]\n",
    "\n",
    "            # Filter files by valid extension\n",
    "            filt_tnames = filter_files_by_ext(all_tnames, valid_exts=valid_exts)\n",
    "            filt_tpaths = [os.path.join(tempdir, name) for name in filt_tnames]\n",
    "\n",
    "            #print('filt_tnames:\\n', filt_tnames, sep='')\n",
    "\n",
    "            # Repopulates the 'proc_files_temp' table\n",
    "            if repopulate_temp_file_table(filt_tpaths):\n",
    "                raise AssertionError(\"Could not repopulate\"\\\n",
    "                                   + \"'proc_files_temp' table.\")\n",
    "\n",
    "            # Obtains the processed files from the ProcessedFiles table\n",
    "            # proc_files = glb.sqla_session.query(ProcessedFiles)\n",
    "\n",
    "            # \n",
    "            query  = select(ProcessedFiles.filename,\n",
    "                            ProcessedFilesTemp.filename).join(\\\n",
    "                            ProcessedFilesTemp, ProcessedFiles.filesize ==\\\n",
    "                            ProcessedFilesTemp.filesize)\n",
    "            result = glb.sqla_session.execute(query)\n",
    "            invalid_names = [tup[1] for tup in result.all()]\n",
    "            print('invalid names:'.ljust(14), invalid_names)\n",
    "\n",
    "            # Loops through each file extracted in the temporary directory\n",
    "            for i, tname, tpath in zip(range(0, len(filt_tnames)), filt_tnames,\n",
    "                                             filt_tpaths):\n",
    "                # ------------------------- File Check -------------------------\n",
    "                # \n",
    "\n",
    "\n",
    "                raise AssertionError('DEBUGGING')\n",
    "\n",
    "                skip_this_file = file_is_not_unique(tpath, proc_qry=proc_files)\n",
    "\n",
    "                # Skips the current file if skip_this_file=True\n",
    "                if skip_this_file:\n",
    "                    print(f'File skipped (file check failed): {tpath}')\n",
    "                    skipped_files.append(tpath)\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # ------------------------ Auto renaming -----------------------\n",
    "                # Checks if the current file name matches any of the other\n",
    "                # files, renaming them using an unique id if 'auto_rename' is\n",
    "                # True. If 'auto_rename' is False (and file requires renaming)\n",
    "                # skip this file and add it to the skipped file names list.\n",
    "                if tname in all_fnames:\n",
    "                    if auto_rename:\n",
    "                        new_name = str(uuid4()) + '.' + tname.split('.')[-1] # uid.extension\n",
    "                    else:\n",
    "                        print(f'File skipped (file exists + no auto rename): {tpath}')\n",
    "                        skipped_files.append(tpath)\n",
    "                        continue\n",
    "\n",
    "                # Otherwise, dont rename it\n",
    "                else:\n",
    "                    new_name = tname\n",
    "\n",
    "                # Move file to appropriate directory\n",
    "                new_fp = os.path.join(image_dir, new_name)\n",
    "                old_fp = os.path.join(tempdir, tname)\n",
    "                sh_move(old_fp, new_fp)\n",
    "\n",
    "    return skipped_files\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def create_database_from_zip(myfile, params, image_dir, auto_rename,\n",
    "                    table_names = ['person', 'representation', 'proc_files']):\n",
    "    \"\"\"\n",
    "    API endpoint: create_database_from_zip()\n",
    "\n",
    "    Creates an SQLite database from a zip file. The zip file is expected to\n",
    "    contain image files in any of the following formats: .jpg, .png, .npy.\n",
    "\n",
    "    The images in the zip file are extracted to a temporary directory. Any image\n",
    "    with the same name of another image in the 'image directory' is either\n",
    "    renamed (auto_rename=True) or skipped (auto_rename=False). Renamed images\n",
    "    are renamed using a random unique object identifier obtained by uuid4() from\n",
    "    the uuid library.\n",
    "\n",
    "    Parameters:\n",
    "    - myfile: a zip file\n",
    "\n",
    "    - params: a structure with the following parameters:\n",
    "        1. detector_name  - name of face detector model [string].\n",
    "        2. verifier_names - list of names of face verifier models [list of\n",
    "                            strings].\n",
    "        3. align          - perform face alignment flag (default=True)\n",
    "                            [boolean].\n",
    "        4. normalization  - name of image normalization [string].\n",
    "        5. auto_grouping  - toggles whether Representations should be grouped /\n",
    "                            clusted automatically using the DBSCAN algorithm\n",
    "                            (default=True) [boolean].\n",
    "        6. eps            - maximum distance between two samples for one to be\n",
    "                            considered as in the neighborhood of the other. This\n",
    "                            is the most important DBSCAN parameter to choose\n",
    "                            appropriately for the specific data set and distance\n",
    "                            function (default=0.5) [float].\n",
    "        7. min_samples    - the number of samples (or total weight) in a\n",
    "                            neighborhood for a point to be considered as a core\n",
    "                            point. This includes the point itself\n",
    "                            (min_samples=2) [integer].\n",
    "        8. metric         - the metric used when calculating distance between\n",
    "                            instances in a feature array. It must be an option\n",
    "                            allowed by sklearn.metrics.pairwise_distances\n",
    "                            (default='cosine') [string].\n",
    "        9. pct            - used to filter faces which are smaller than this\n",
    "                            percentage of the original image's area (width x\n",
    "                            height) [float].\n",
    "       10. check_models   - toggles if the function should check if all desired\n",
    "                            face detector & verifiers are correctly loaded. If\n",
    "                            they are not, builds them from scratch, exitting if\n",
    "                            the building fails [boolean].\n",
    "       11. verbose        - output messages to server's console [boolean].\n",
    "\n",
    "        [Example] JSON schema:\n",
    "        {\n",
    "          \"detector_name\": \"retinaface\",\n",
    "          \"verifier_names\": [\"ArcFace\"],\n",
    "          \"align\": true,\n",
    "          \"normalization\": \"base\",\n",
    "          \"auto_grouping\": true,\n",
    "          \"eps\": 0.5,\n",
    "          \"min_samples\": 2,\n",
    "          \"metric\": \"cosine\",\n",
    "          \"pct\": 0.02,\n",
    "          \"check_models\": true,\n",
    "          \"verbose\": false\n",
    "        }\n",
    "\n",
    "    - image_dir   : full path to directory containing images (string,\n",
    "                     default: <glb.IMG_DIR>)\n",
    "\n",
    "    - db_dir      : full path to directory containing saved database (string,\n",
    "                     default: <glb.RDB_DIR>)\n",
    "\n",
    "    - auto_rename : flag to force auto renaming of images in the zip file with\n",
    "                     names that match images already in the image directory\n",
    "                     (boolean, default: True)\n",
    "\n",
    "    - force_create: flag to force database creation even if one already exists,\n",
    "                     overwritting the old one (boolean, default: True)\n",
    "\n",
    "    Output:\\n\n",
    "        JSON-encoded dictionary with the following key/value pairs is returned:\n",
    "            1. length: length of the newly created database OR of the currently\n",
    "                loaded one if this process is skipped (i.e. force_create=False\n",
    "                with existing database loaded)\n",
    "            \n",
    "            2. message: informative message string\n",
    "    \"\"\"   \n",
    "    # Initialize output message\n",
    "    output_msg = ''\n",
    "\n",
    "    # If image directory provided is None or is not a directory, use default\n",
    "    # directory\n",
    "    if not image_dir or not os.path.isdir(image_dir):\n",
    "        global img_dir\n",
    "        output_msg += 'Image dir is None, does not exist or is not a '\\\n",
    "                   +  'directory. Using default directory instead.\\n'\n",
    "        image_dir = img_dir\n",
    "\n",
    "    # Database does not exist\n",
    "    if  database_is_empty(glb.sqla_engine):\n",
    "        # Do nothing, but set message\n",
    "        output_msg += 'Database does not exist! '\\\n",
    "                   +  'Please create one before using this endpoint.\\n'\n",
    "\n",
    "    # Face Representation table does not exist\n",
    "    elif not all_tables_exist(glb.sqla_engine, table_names):\n",
    "        # Do nothing, but set message\n",
    "        output_msg += \"Face representation table ('representation') \"\\\n",
    "                   +  'does not exist! Please ensure that this table exists '\\\n",
    "                   +  'before using this endpoint.\\n'\n",
    "\n",
    "    # Otherwise (database is not empty and table exists), \n",
    "    else:\n",
    "        # Initialize dont_skip flag as True\n",
    "        dont_skip   = True\n",
    "\n",
    "        # Extract zip files\n",
    "        output_msg += 'Extracting images in zip:'\n",
    "\n",
    "        print('Entered else (main body)')\n",
    "\n",
    "        try:\n",
    "            # Process the zip file containing the image files\n",
    "            skipped_files = process_image_zip_file2(myfile, image_dir,\n",
    "                                                    auto_rename=auto_rename)\n",
    "            output_msg += ' success! '\n",
    "\n",
    "        except Exception as excpt:\n",
    "            dont_skip   = False\n",
    "            output_msg += f' failed (reason: {excpt}).'\n",
    "\n",
    "        raise AssertionError('FOR DEBUGGING - assertion error break!')\n",
    "\n",
    "        # Processes face images from the image directory provided if 'dont_skip'\n",
    "        # is True\n",
    "        if dont_skip:\n",
    "            output_msg += 'Creating database: '\n",
    "            records = process_faces_from_dir(image_dir, glb.models, glb.models,\n",
    "                            detector_name  = params.detector_name,\n",
    "                            verifier_names = params.verifier_names,\n",
    "                            normalization  = params.normalization,\n",
    "                            align          = params.align,\n",
    "                            auto_grouping  = params.auto_grouping,\n",
    "                            eps            = params.eps,\n",
    "                            min_samples    = params.min_samples,\n",
    "                            metric         = params.metric,\n",
    "                            pct            = params.pct,\n",
    "                            check_models   = params.check_models,\n",
    "                            verbose        = params.verbose)\n",
    "        \n",
    "            # Commits the records and updates the message\n",
    "            glb.sqla_session.commit()\n",
    "            output_msg += ' success!'\n",
    "        else:\n",
    "            records = []\n",
    "\n",
    "    return {'n_records':len(records), 'n_skipped':len(skipped_files),\n",
    "            'skipped_files':skipped_files, 'message':output_msg}\n",
    "\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing face images: 100%|██████████| 4/4 [00:20<00:00,  5.12s/it]\n"
     ]
    }
   ],
   "source": [
    "do_process_faces_from_dir = False\n",
    "if do_process_faces_from_dir:\n",
    "    records = process_faces_from_dir(test_dir, detector_models, verifier_models,\n",
    "                                      detector_name  = detector_name,\n",
    "                                      verifier_names = verifier_names,\n",
    "                                      normalization  = normalization,\n",
    "                                      align          = align,\n",
    "                                      auto_grouping  = auto_grouping,\n",
    "                                      eps            = eps,\n",
    "                                      min_samples    = min_samples,\n",
    "                                      metric         = metric,\n",
    "                                      pct            = pct,\n",
    "                                      check_models   = check_models,\n",
    "                                      verbose        = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:        [('img01.jpg', 'img01.jpg'), ('img02.jpg', 'img02.jpg'), ('img03.jpg', 'img03.jpg'), ('img53.jpg', 'img53.jpg')]\n",
      "invalid names: ['img01.jpg', 'img02.jpg', 'img03.jpg', 'img53.jpg']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "DEBUGGING",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/rpessoa/projects/loki/developing_nb.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m test_process_zip_file:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m     myfile \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mapi/data/test1a.zip\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m     skipped_files \u001b[39m=\u001b[39m process_image_zip_file2(myfile, image_dir, auto_rename\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000013vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000013vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(skipped_files), \u001b[39m'\u001b[39m\u001b[39mskipped files:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/rpessoa/projects/loki/developing_nb.ipynb Cell 13'\u001b[0m in \u001b[0;36mprocess_image_zip_file2\u001b[0;34m(myfile, image_dir, auto_rename, valid_exts)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=174'>175</a>\u001b[0m \u001b[39m# Loops through each file extracted in the temporary directory\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=175'>176</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, tname, tpath \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(filt_tnames)), filt_tnames,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=176'>177</a>\u001b[0m                                  filt_tpaths):\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=177'>178</a>\u001b[0m     \u001b[39m# ------------------------- File Check -------------------------\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=178'>179</a>\u001b[0m     \u001b[39m# \u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=181'>182</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDEBUGGING\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=183'>184</a>\u001b[0m     skip_this_file \u001b[39m=\u001b[39m file_is_not_unique(tpath, proc_qry\u001b[39m=\u001b[39mproc_files)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/rpessoa/projects/loki/developing_nb.ipynb#ch0000012vscode-remote?line=185'>186</a>\u001b[0m     \u001b[39m# Skips the current file if skip_this_file=True\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: DEBUGGING"
     ]
    }
   ],
   "source": [
    "test_process_zip_file = True\n",
    "if test_process_zip_file:\n",
    "    myfile = 'api/data/test1a.zip'\n",
    "    skipped_files = process_image_zip_file2(myfile, image_dir, auto_rename=True)\n",
    "\n",
    "    print('')\n",
    "    print(len(skipped_files), 'skipped files:')\n",
    "    for skpd_file in skipped_files:\n",
    "        print(f'  > {skpd_file}')\n",
    "    if len(skipped_files) == 0:\n",
    "        print('  None')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zip_endpoint = False\n",
    "if test_zip_endpoint:\n",
    "    myfile = 'api/data/test1.zip'\n",
    "    output = create_database_from_zip(myfile, params, image_dir, auto_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filecmp import cmp\n",
    "import imagesize\n",
    "\n",
    "tst_dir = 'api/data/img'\n",
    "tst_pth = 'api/data/img/img34_copy_test.jpg'\n",
    "tst_pth = 'api/data/img/img12.jpg'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def remove_img_file_duplicates(trgt_dir, dont_delete=False):\n",
    "    \"\"\"\n",
    "    Detects and removes (if dont_delete=False) all duplicate image files in a\n",
    "    target directory 'trgt_dir'. Also returns a list with the name of all\n",
    "    duplicate files, regardless if they were deleted or not. The algorithm works\n",
    "    in the following way:\n",
    "\n",
    "        1. The full path and file size of all files (in the directory) are\n",
    "            obtained. A list with all unique file sizes is calculated.\n",
    "\n",
    "        2. For each file size in the unique file size list:\n",
    "            2.1. The indicies of all files with a matching file size are\n",
    "                  obtained.\n",
    "\n",
    "            2.2. If there are multiple matches, the first file (corresponding to\n",
    "                  the first index) is set as the reference file for comparison.\n",
    "                  Its width and height are calculated without loading the entire\n",
    "                  image to memory.\n",
    "\n",
    "            2.3. Every other match is compared to the reference file. The\n",
    "                  comparison is made by using filecmp.cmp() (with\n",
    "                  shallow=False). Their widths and heights are also calculated\n",
    "                  and compared to the reference image's width and height.\n",
    "\n",
    "            2.4. If any match is deemed the same (filecmp.cmp() results in True\n",
    "                  and has the same width and height), the matching file is\n",
    "                  considered a duplicate and is deleted (unless\n",
    "                  dont_delete=True). The file's name is also stored in the\n",
    "                  duplicate file names' list.\n",
    "\n",
    "        3. Returns a list with the names of all duplicate files (regardless if\n",
    "            they were deleted or not).\n",
    "\n",
    "    Inputs:\n",
    "        1. trgt_dir    - path to target directory [string].\n",
    "\n",
    "        2. dont_delete - toggles if the function should delete the duplicate\n",
    "                          files or not [boolean, default=False].\n",
    "\n",
    "    Output:\n",
    "        1. Returns the names of all duplicate files (regardless if they were\n",
    "            deleted or not) [list of strings].\n",
    "\n",
    "    Signature:\n",
    "        dup_file_names = remove_img_file_duplicates(trgt_dir, dont_delete=False)\n",
    "    \"\"\"\n",
    "    # Initialize duplicate files' name list\n",
    "    dup_files = []\n",
    "\n",
    "    # Obtains all file full paths ('all_files'), their file sizes ('all_sizes')\n",
    "    # and a list of all unique file sizes ('unq_sizes')\n",
    "    all_files = [os.path.join(trgt_dir, pth) for pth in os.listdir(trgt_dir)]\n",
    "    all_sizes = np.array([os.path.getsize(pth) for pth in all_files])\n",
    "    unq_sizes = np.unique(all_sizes)\n",
    "\n",
    "    # Loops through all unique file sizes\n",
    "    for sze in unq_sizes:\n",
    "        # Gets the indices of all files with the same current file size\n",
    "        ii = np.where(all_sizes == sze)[0]\n",
    "\n",
    "        # If there are multiple matches, compare them to see if there are\n",
    "        # duplicates. Otherwise, just continue\n",
    "        if len(ii) > 1:\n",
    "            # Sets the first index (file) as a reference file (for comparison)\n",
    "            # and obtains their width and height\n",
    "            refw, refh = imagesize.get(all_files[ii[0]])\n",
    "\n",
    "            # Loops through each remaining file index\n",
    "            for i in ii[1:]:\n",
    "                # Calculates the current matched file's width and height\n",
    "                wi, hi = imagesize.get(all_files[i])\n",
    "\n",
    "                # Files have the same size, content and image size\n",
    "                if cmp(all_files[ii[0]], all_files[i], shallow=False)\\\n",
    "                    and refw == wi and refh == hi:\n",
    "                    # Appends the duplicate file's name\n",
    "                    dup_files.append(all_files[i])\n",
    "\n",
    "                    # Removes the duplicate file if dont_delete=False\n",
    "                    if not dont_delete:\n",
    "                        os.remove(all_files[i])\n",
    "\n",
    "    return dup_files\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def img_file_is_duplicate(img_path, file_fps, file_sizes):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initializes is_duplicate flag as False (assumes its unique)\n",
    "    is_duplicate = False\n",
    "\n",
    "    # Ensures the file_sizes list is a numpy array\n",
    "    if not isinstance(file_sizes, np.ndarray):\n",
    "        file_sizes = np.array(file_sizes)\n",
    "\n",
    "    # Gets the indices of all files with the file as the current file's size\n",
    "    ii = np.where(file_sizes == os.path.getsize(img_path))[0]\n",
    "\n",
    "    # If there are multiple matches, compare them to see if there are\n",
    "    # duplicates. Otherwise, this file is unique so just return False\n",
    "    if len(ii) > 1:\n",
    "        # Gets the width and height of the input file \n",
    "        refw, refh = imagesize.get(img_path)\n",
    "\n",
    "        # Loops through each matched files' index\n",
    "        for i in ii:\n",
    "            # Calculates the current matched file's width and height\n",
    "            wi, hi = imagesize.get(file_fps[i])\n",
    "\n",
    "            # Files have the same size, content and image size, so this file is\n",
    "            # a duplicate\n",
    "            if cmp(img_path, file_fps[i], shallow=False) and refw == wi\\\n",
    "                and refh == hi:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "\n",
    "    return is_duplicate\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "dup_file_names = remove_img_file_duplicates(tst_dir, dont_delete=True)\n",
    "\n",
    "print('Duplicate files:')\n",
    "for name in dup_file_names:\n",
    "    print('  > ', name)\n",
    "if len(dup_file_names) == 0:\n",
    "    print('  None')\n",
    "\n",
    "print('\\n', '-'*79, '\\n', sep='')\n",
    "files_fps = [os.path.join(tst_dir, pth) for pth in os.listdir(tst_dir)]\n",
    "files_sizes = np.array([os.path.getsize(pth) for pth in files_fps])\n",
    "\n",
    "print('File is duplicate:',\n",
    "        img_file_is_duplicate(tst_pth, files_fps, files_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b8b4124e34c7aef84f71e2d1adc63dc40a11e5c939516b2a86977f9bc5888db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('loki_env2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
